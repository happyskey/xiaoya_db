#!/usr/bin/env python3
import argparse
import logging
import sys, os
import urllib.error
import urllib.parse
import urllib.request
from urllib.parse import urljoin, urlparse, unquote, quote
import aiohttp.client_exceptions
from bs4 import BeautifulSoup
from datetime import datetime
import random
import re
import gzip
import asyncio
import aiofiles
import aiohttp
from aiohttp import ClientSession, TCPConnector
import aiosqlite
import aiofiles.os as aio_os

# é…ç½®æ—¥å¿—è¾“å‡ºæ ¼å¼ï¼Œæ—¶é—´æ ¼å¼ä¸º YYYY-MM-DD HH:MM:SSï¼Œè¾“å‡ºåˆ°æ ‡å‡†è¾“å‡º
logging.basicConfig(
    format="%(asctime)s %(levelname)s %(message)s",
    level=logging.INFO,
    datefmt="%Y-%m-%d %H:%M:%S",
    stream=sys.stdout,
)
logger = logging.getLogger("emd")
logging.getLogger("chardet.charsetprober").disabled = True  # ç¦ç”¨å­—ç¬¦é›†æ¢æµ‹æ—¥å¿—

# å®šä¹‰æ‰€æœ‰æ”¯æŒçš„åª’ä½“è·¯å¾„ï¼ˆURL ç¼–ç ï¼‰ï¼Œç”¨äºæŒ‡å®šçˆ¬å–çš„æ–‡ä»¶å¤¹
s_paths_all = [
    quote("æµ‹è¯•/"),  # 512
    quote("åŠ¨æ¼«/"),  # 256
    quote("æ¯æ—¥æ›´æ–°/"),  # 128
    quote("ç”µå½±/"),  # 64
    quote("ç”µè§†å‰§/"),  # 32
    quote("çºªå½•ç‰‡/"),  # 16
    quote("çºªå½•ç‰‡ï¼ˆå·²åˆ®å‰Šï¼‰/"),  # 8
    quote("ç»¼è‰º/"),  # 4
    quote("å›½äº§å‰§ä¸“å±/"),  # 4
    quote("éŸ³ä¹/"),  # 2
    quote("ğŸ“ºç”»è´¨æ¼”ç¤ºæµ‹è¯•ï¼ˆ4Kï¼Œ8Kï¼ŒHDRï¼ŒDolbyï¼‰/"),  # 1
]

# å®šä¹‰å¿…é¡»æ£€æŸ¥çš„åª’ä½“æ–‡ä»¶å¤¹ï¼ˆç”¨äº test_media_folderï¼‰
t_paths = [
    quote("æ¯æ—¥æ›´æ–°/"),
    quote("å›½äº§å‰§ä¸“å±/"),
]

# å®šä¹‰é»˜è®¤çˆ¬å–çš„è·¯å¾„ï¼ˆå¦‚æœæœªæŒ‡å®š --paths æˆ– --allï¼‰
s_paths = [
    quote("æ¯æ—¥æ›´æ–°/"),
    quote("å›½äº§å‰§ä¸“å±/"),
]

# å®šä¹‰é»˜è®¤çš„ URL æ± ï¼Œç”¨äºçˆ¬å–åª’ä½“æ–‡ä»¶
s_pool = [
    "http://happyskey.top:8108/",
]

# å®šä¹‰è¦å¿½ç•¥çš„æ–‡ä»¶å¤¹
s_folder = [".sync"]

# å®šä¹‰è¦å¿½ç•¥çš„æ–‡ä»¶æ‰©å±•åï¼ˆå¦‚å­—å¹•æ–‡ä»¶ï¼‰
s_ext = [".ass", ".srt", ".ssa"]

# é…ç½®è‡ªå®šä¹‰ User-Agentï¼Œç»•è¿‡ Cloudflare é™åˆ¶
CUSTOM_USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36"
opener = urllib.request.build_opener()
opener.addheaders = [("User-Agent", CUSTOM_USER_AGENT)]
urllib.request.install_opener(opener)

def pick_a_pool_member(url_list):
    """ä» URL æ± ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªå¯ç”¨çš„ URL"""
    random.shuffle(url_list)
    for member in url_list:
        try:
            logger.debug("æµ‹è¯• URL: %s", member)
            response = urllib.request.urlopen(member)
            if response.getcode() == 200:
                content = response.read()
                try:
                    content_decoded = content.decode("utf-8")
                    if "æ¯æ—¥æ›´æ–°" in content_decoded:
                        logger.info("é€‰æ‹© URL: %s", member)
                        return member
                    else:
                        logger.info("URL %s ä¸åŒ…å« 'æ¯æ—¥æ›´æ–°'", member)
                except UnicodeDecodeError:
                    logger.info("URL %s è¿”å›é UTF-8 å†…å®¹", member)
        except Exception as e:
            logger.info("è®¿é—® URL %s å¤±è´¥: %s", member, e)
    return None

def current_amount(url, media, paths):
    """ç»Ÿè®¡è¿œç¨‹ scan.list.gz ä¸­æŒ‡å®šè·¯å¾„ä¸‹çš„æ–‡ä»¶æ•°é‡"""
    listfile = os.path.join(media, ".scan.list.gz")
    try:
        res = urllib.request.urlretrieve(url, listfile)
        with gzip.open(listfile) as response:
            pattern = r"^\d{4}-\d{2}-\d{2} \d{2}:\d{2} \/(.*)$"
            hidden_pattern = r"^.*?\/\..*$"
            matching_lines = 0
            for line in response:
                try:
                    line = line.decode(encoding="utf-8").strip()
                    match = re.match(pattern, line)
                    if match:
                        file = match.group(1)
                        if any(file.startswith(unquote(path)) for path in paths):
                            if not re.match(hidden_pattern, file) and not file.lower().endswith(".txt"):
                                matching_lines += 1
                except:
                    logger.error("è§£ç è¡Œå¤±è´¥: %s", line)
        return matching_lines
    except urllib.error.URLError as e:
        print("é”™è¯¯:", e)
        return -1

async def fetch_html(url, session, **kwargs) -> str:
    """å¼‚æ­¥è·å– URL çš„ HTML å†…å®¹"""
    semaphore = kwargs["semaphore"]
    async with semaphore:
        async with session.request(method="GET", url=url) as resp:
            logger.debug("è¯·æ±‚å¤´ [%s]: [%s]", unquote(url), resp.request_info.headers)
            resp.raise_for_status()
            logger.debug("å“åº”å¤´ [%s]: [%s]", unquote(url), resp.headers)
            logger.debug("æ”¶åˆ°å“åº” [%s] for URL: %s", resp.status, unquote(url))
            try:
                text = await resp.text()
                return text
            except UnicodeDecodeError:
                logger.error("URL %s è¿”å›é UTF-8 å†…å®¹", unquote(url))
                return None

async def parse(url, session, max_retries=3, **kwargs) -> set:
    """è§£æ HTML é¡µé¢ï¼Œæå–æ–‡ä»¶å’Œç›®å½•ä¿¡æ¯"""
    global html
    retries = 0
    files = []
    directories = []
    while True:
        if retries < max_retries:
            try:
                html = await fetch_html(url=url, session=session, **kwargs)
                if html is None:
                    logger.debug("æ— æ³•è·å– URL çš„ HTML å†…å®¹: %s", unquote(url))
                    return files, directories
                break
            except aiohttp.ClientResponseError as e:
                logger.error(
                    "aiohttp ClientResponseError for %s [%s]: %s. é‡è¯• (%d/%d)...",
                    unquote(url),
                    getattr(e, "status", None),
                    getattr(e, "message", None),
                    retries + 1,
                    max_retries,
                )
                retries += 1
            except (
                aiohttp.ClientError,
                aiohttp.http_exceptions.HttpProcessingError,
                aiohttp.ClientPayloadError,
            ) as e:
                logger.error(
                    "aiohttp å¼‚å¸¸ for %s [%s]: %s",
                    unquote(url),
                    getattr(e, "status", None),
                    getattr(e, "message", None),
                )
                return files, directories
            except Exception as e:
                logger.exception("é aiohttp å¼‚å¸¸: %s", getattr(e, "__dict__", {}))
                return files, directories
        else:
            logger.error("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ŒURL %s è¯·æ±‚å¤±è´¥", unquote(url))
            return files, directories

    soup = BeautifulSoup(html, "html.parser")
    for link in soup.find_all("a"):
        href = link.get("href")
        # è·³è¿‡æ— æ•ˆæˆ–ç‰¹æ®Šé“¾æ¥ï¼ˆå¦‚æ’åºé“¾æ¥ã€email-protectionï¼‰
        if not href or href.startswith("?C=") or "/cdn-cgi/l/email-protection" in href:
            continue
        if (
            href != "../"
            and not href.endswith("/")
            and not href.lower().endswith(".txt")
            and href != "scan.list"
        ):
            try:
                abslink = urljoin(url, href)
                filename = unquote(urlparse(abslink).path)
                # éªŒè¯ next_sibling æ˜¯å¦æœ‰æ•ˆ
                if not link.next_sibling or not link.next_sibling.strip():
                    logger.debug("URL %s æ— æœ‰æ•ˆ next_sibling", href)
                    continue
                # æå–æ—¶é—´æˆ³å¹¶éªŒè¯
                timestamp_str = link.next_sibling.strip().split()[:2]
                if len(timestamp_str) != 2:
                    logger.debug("URL %s çš„æ—¶é—´æˆ³æ•°æ®æ— æ•ˆ", href)
                    continue
                timestamp = datetime.strptime(" ".join(timestamp_str), "%Y-%m-%d %H:%M")
                timestamp_unix = int(timestamp.timestamp())
                filesize = link.next_sibling.strip().split()[2]
                files.append((abslink, filename, timestamp_unix, filesize))
            except (urllib.error.URLError, ValueError) as e:
                logger.exception("è§£æ URL é”™è¯¯: %s", href)
                continue
            except Exception as e:
                logger.exception("URL %s å‡ºç°æ„å¤–é”™è¯¯: %s", href, e)
                continue
        elif href != "../" and not href.lower().endswith(".txt"):
            directories.append(urljoin(url, href))
    return files, directories

async def need_download(file, **kwargs):
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦éœ€è¦ä¸‹è½½ï¼ˆåŸºäºå­˜åœ¨æ€§ã€æ—¶é—´æˆ³å’Œæ–‡ä»¶å¤§å°ï¼‰"""
    url, filename, timestamp, filesize = file
    file_path = os.path.join(kwargs["media"], filename.lstrip("/"))
    if not os.path.exists(file_path):
        logger.debug("%s ä¸å­˜åœ¨", file_path)
        return True
    elif file_path.endswith(".nfo"):
        if not kwargs["nfo"]:
            return False
    current_filesize = os.path.getsize(file_path)
    current_timestamp = os.path.getmtime(file_path)
    logger.debug("%s çš„æ—¶é—´æˆ³: %s, å¤§å°: %s", filename, timestamp, filesize)
    if int(filesize) == int(current_filesize) and int(timestamp) <= int(current_timestamp):
        return False
    logger.debug("%s çš„å½“å‰æ—¶é—´æˆ³: %s, å½“å‰å¤§å°: %s", filename, current_timestamp, current_filesize)
    return True

async def download(file, session, **kwargs):
    """å¼‚æ­¥ä¸‹è½½æ–‡ä»¶å¹¶ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„"""
    url, filename, timestamp, filesize = file
    semaphore = kwargs["semaphore"]
    async with semaphore:
        try:
            async with session.get(url) as response:
                if response.status == 200:
                    file_path = os.path.join(kwargs["media"], filename.lstrip("/"))
                    os.umask(0)
                    os.makedirs(os.path.dirname(file_path), mode=0o777, exist_ok=True)
                    async with aiofiles.open(file_path, "wb") as f:
                        logger.debug("å¼€å§‹å†™å…¥æ–‡ä»¶: %s", filename)
                        await f.write(await response.content.read())
                        logger.debug("å®Œæˆå†™å…¥æ–‡ä»¶: %s", filename)
                    os.chmod(file_path, 0o777)
                    logger.info("ä¸‹è½½å®Œæˆ: %s", filename)
                else:
                    logger.error("ä¸‹è½½å¤±è´¥: %s [å“åº”ç : %s]", filename, response.status)
        except Exception as e:
            logger.exception("ä¸‹è½½å¼‚å¸¸: %s", e)

async def download_files(files, session, **kwargs):
    """æ‰¹é‡ä¸‹è½½æ–‡ä»¶"""
    download_tasks = set()
    for file in files:
        if await need_download(file, **kwargs) is True:
            task = asyncio.create_task(download(file, session, **kwargs))
            task.add_done_callback(download_tasks.discard)
            download_tasks.add(task)
            if len(download_tasks) > 100:
                await asyncio.gather(*download_tasks)
    await asyncio.gather(*download_tasks)

async def create_table(conn):
    """åˆ›å»ºæ•°æ®åº“è¡¨ï¼Œç”¨äºå­˜å‚¨æ–‡ä»¶ä¿¡æ¯"""
    try:
        async with conn.execute("""
            CREATE TABLE IF NOT EXISTS files (
                filename TEXT,
                timestamp INTEGER NULL,
                filesize INTEGER NULL)
        """):
            pass
    except Exception as e:
        logger.error("æ— æ³•åˆ›å»ºæ•°æ®åº“: %s", e)
        sys.exit(1)

async def insert_files(conn, items):
    """å°†æ–‡ä»¶ä¿¡æ¯æ’å…¥æ•°æ®åº“"""
    await conn.executemany("INSERT OR REPLACE INTO files VALUES (?, ?, ?)", items)
    await conn.commit()

async def exam_file(file, media):
    """æ£€æŸ¥æœ¬åœ°æ–‡ä»¶çš„çŠ¶æ€ï¼ˆè·¯å¾„ã€æ—¶é—´æˆ³ã€å¤§å°ï¼‰"""
    stat = await aio_os.stat(file)
    return file[len(media) :], int(stat.st_mtime), stat.st_size

def process_folder(folder, media):
    """éå†æ–‡ä»¶å¤¹ï¼Œæ”¶é›†ééšè—æ–‡ä»¶å’Œéå­—å¹•æ–‡ä»¶çš„è·¯å¾„"""
    all_items = []
    for root, dirs, files in os.walk(folder, topdown=False):
        dirs[:] = [d for d in dirs if d not in s_folder]
        for file in files:
            if not file.startswith(".") and not file.lower().endswith(tuple(s_ext)):
                file_path = os.path.join(root, file)
                try:
                    file_path.encode("utf-8")
                    relative_path = file_path[len(media) :]
                except UnicodeEncodeError:
                    logging.error("æ–‡ä»¶åé UTF-8 ç¼–ç : %s", file_path)
                    relative_path = None
                if relative_path:
                    all_items.append((relative_path, None, None))
    return all_items

def remove_empty_folders(paths, media):
    """åˆ é™¤æŒ‡å®šè·¯å¾„ä¸‹çš„ç©ºæ–‡ä»¶å¤¹"""
    for path in paths:
        for root, dirs, files in os.walk(unquote(os.path.join(media, path)), topdown=False):
            dirs[:] = [d for d in dirs if d not in s_folder]
            if not dirs and not files:
                try:
                    os.rmdir(root)
                    logger.info("åˆ é™¤ç©ºæ–‡ä»¶å¤¹: %s", root)
                except OSError as e:
                    logger.error("æ— æ³•åˆ é™¤æ–‡ä»¶å¤¹ %s: %s", root, e)

async def generate_localdb(db, media, paths):
    """ç”Ÿæˆæœ¬åœ°æ•°æ®åº“ï¼Œè®°å½•åª’ä½“æ–‡ä»¶ä¿¡æ¯"""
    logger.warning("æ­£åœ¨ç”Ÿæˆæœ¬åœ°æ•°æ®åº“... è§†ç£ç›˜ I/O æ€§èƒ½å¯èƒ½è€—æ—¶è¾ƒé•¿ï¼Œè¯·å‹¿ä¸­æ–­...")
    async with aiosqlite.connect(db) as conn:
        await create_table(conn)
        for path in paths:
            logger.info("å¤„ç†è·¯å¾„: %s", unquote(os.path.join(media, path)))
            items = process_folder(unquote(os.path.join(media, path)), media)
            await insert_files(conn, items)
        total_items_count = await get_total_items_count(conn)
        logger.info("æœ¬åœ°ç£ç›˜ä¸Šæœ‰ %d ä¸ªæ–‡ä»¶", total_items_count)

async def get_total_items_count(conn):
    """è·å–æ•°æ®åº“ä¸­æ–‡ä»¶æ€»æ•°"""
    async with conn.execute("SELECT COUNT(*) FROM files") as cursor:
        result = await cursor.fetchone()
        total_count = result[0] if result else 0
    return total_count

async def write_one(url, session, db_session, **kwargs) -> list:
    """å¤„ç†å•ä¸ª URLï¼Œè§£æå¹¶ä¸‹è½½æ–‡ä»¶ï¼Œè¿”å›å­ç›®å½•åˆ—è¡¨"""
    if urlparse(url).path == "/":
        directories = []
        for path in kwargs["paths"]:
            directories.append(urljoin(url, path))
        return directories
    files, directories = await parse(url=url, session=session, **kwargs)
    if not files:
        return directories
    if kwargs["media"]:
        await download_files(files=files, session=session, **kwargs)
    if db_session:
        items = []
        for file in files:
            items.append(file[1:])
        await db_session.executemany("INSERT OR REPLACE INTO files VALUES (?, ?, ?)", items)
        await db_session.commit()
        logger.debug("å·²å†™å…¥ URL çš„ç»“æœ: %s", unquote(url))
    return directories

async def bulk_crawl_and_write(url, session, db_session, depth=0, **kwargs) -> None:
    """é€’å½’çˆ¬å– URL å’Œå…¶å­ç›®å½•"""
    tasks = set()
    directories = await write_one(url=url, session=session, db_session=db_session, **kwargs)
    for url in directories:
        task = asyncio.create_task(
            bulk_crawl_and_write(
                url=url,
                session=session,
                db_session=db_session,
                depth=depth + 1,
                **kwargs,
            )
        )
        task.add_done_callback(tasks.discard)
        tasks.add(task)
        if depth == 0:
            await asyncio.gather(*tasks)
    await asyncio.gather(*tasks)

async def compare_databases(localdb, tempdb, total_amount):
    """æ¯”è¾ƒæœ¬åœ°å’Œä¸´æ—¶æ•°æ®åº“ï¼Œæ‰¾å‡ºå·²åˆ é™¤çš„æ–‡ä»¶"""
    async with aiosqlite.connect(localdb) as conn1, aiosqlite.connect(tempdb) as conn2:
        cursor1 = await conn1.cursor()
        cursor2 = await conn2.cursor()
        await cursor1.execute("SELECT filename FROM files")
        local_filenames = set(filename[0] for filename in await cursor1.fetchall())
        await cursor2.execute("SELECT filename FROM files")
        temp_filenames = set(filename[0] for filename in await cursor2.fetchall())
        gap = abs(len(temp_filenames) - total_amount)
        if gap < 10 and total_amount > 0:
            if not gap == 0:
                logger.warning(
                    "æ–‡ä»¶æ€»æ•°ä¸åŒ¹é…: %d -> %dï¼Œå·®è· %d å°äº 10ï¼Œç»§ç»­æ¸…ç†...",
                    total_amount,
                    len(temp_filenames),
                    abs(len(temp_filenames) - total_amount),
                )
            diff_filenames = local_filenames - temp_filenames
            return diff_filenames
        else:
            logger.error("æ–‡ä»¶æ€»æ•°ä¸åŒ¹é…: %d -> %dï¼Œè·³è¿‡æ¸…ç†", total_amount, len(temp_filenames))
            return []

async def purge_removed_files(localdb, tempdb, media, total_amount):
    """åˆ é™¤æœ¬åœ°æ•°æ®åº“ä¸­ä¸å­˜åœ¨äºè¿œç¨‹çš„æ–‡ä»¶"""
    for file in await compare_databases(localdb, tempdb, total_amount):
        logger.info("æ¸…ç†æ–‡ä»¶: %s", file)
        try:
            os.remove(media + file)
        except Exception as e:
            logger.error("æ— æ³•åˆ é™¤æ–‡ä»¶ %s: %s", file, e)

def test_media_folder(media, paths):
    """æ£€æŸ¥åª’ä½“ç›®å½•æ˜¯å¦åŒ…å«å¿…è¦çš„å­æ–‡ä»¶å¤¹"""
    t_paths = [os.path.join(media, unquote(path)) for path in paths]
    if all(os.path.exists(os.path.abspath(path)) for path in t_paths):
        return True
    else:
        return False

def load_paths_from_file(path_file):
    """ä»æ–‡ä»¶ä¸­åŠ è½½è·¯å¾„åˆ—è¡¨"""
    paths = []
    try:
        with open(path_file, "r", encoding="utf-8") as file:
            for line in file:
                stripped_line = line.strip()
                if stripped_line:
                    encoded_path = quote(stripped_line)
                    if is_subpath(encoded_path, s_paths_all):
                        paths.append(encoded_path)
                    else:
                        logging.error("æ— æ•ˆè·¯å¾„: %s", unquote(encoded_path))
                        return []
    except Exception as e:
        logging.error("åŠ è½½è·¯å¾„æ–‡ä»¶å¤±è´¥: %s", str(e))
    return paths

def is_subpath(path, base_paths):
    """æ£€æŸ¥è·¯å¾„æ˜¯å¦ä¸ºæŒ‡å®šè·¯å¾„çš„å­è·¯å¾„"""
    for base_path in base_paths:
        if path.startswith(base_path):
            return True
    return False

def get_paths_from_bitmap(bitmap, paths_all):
    """æ ¹æ®ä½å›¾é€‰æ‹©è·¯å¾„"""
    max_bitmap_value = (1 << len(paths_all)) - 1
    if bitmap < 0 or bitmap > max_bitmap_value:
        raise ValueError(f"ä½å›¾å€¼ {bitmap} è¶…å‡ºèŒƒå›´ï¼Œå¿…é¡»åœ¨ 0 åˆ° {max_bitmap_value} ä¹‹é—´")
    selected_paths = []
    binary_representation = bin(bitmap)[2:].zfill(len(paths_all))
    for i, bit in enumerate(binary_representation):
        if bit == "1":
            selected_paths.append(paths_all[i])
    return selected_paths

async def main():
    """ä¸»å‡½æ•°ï¼Œå¤„ç†å‘½ä»¤è¡Œå‚æ•°å¹¶å¯åŠ¨çˆ¬å–"""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--media",
        metavar="<folder>",
        type=str,
        default=None,
        required=True,
        help="å­˜å‚¨ä¸‹è½½åª’ä½“æ–‡ä»¶çš„è·¯å¾„ [é»˜è®¤: %(default)s]",
    )
    parser.add_argument(
        "--count",
        metavar="[number]",
        type=int,
        default=100,
        help="æœ€å¤§å¹¶å‘ HTTP è¯·æ±‚æ•° [é»˜è®¤: %(default)s]",
    )
    parser.add_argument(
        "--debug",
        action=argparse.BooleanOptionalAction,
        type=bool,
        default=False,
        help="å¯ç”¨è¯¦ç»†è°ƒè¯•æ—¥å¿— [é»˜è®¤: %(default)s]",
    )
    parser.add_argument(
        "--db",
        action=argparse.BooleanOptionalAction,
        type=bool,
        default=False,
        help="<éœ€è¦ Python 3.12+> ä¿å­˜åˆ°æ•°æ®åº“ [é»˜è®¤: %(default)s]",
    )
    parser.add_argument(
        "--nfo",
        action=argparse.BooleanOptionalAction,
        type=bool,
        default=False,
        help="ä¸‹è½½ NFO æ–‡ä»¶ [é»˜è®¤: %(default)s]",
    )
    parser.add_argument(
        "--url",
        metavar="[url]",
        type=str,
        default=None,
        help="ä¸‹è½½è·¯å¾„ [é»˜è®¤: %(default)s]",
    )
    parser.add_argument(
        "--purge",
        action=argparse.BooleanOptionalAction,
        type=bool,
        default=True,
        help="æ¸…ç†å·²åˆ é™¤çš„æ–‡ä»¶ [é»˜è®¤: %(default)s]",
    )
    parser.add_argument(
        "--all",
        action=argparse.BooleanOptionalAction,
        type=bool,
        default=False,
        help="ä¸‹è½½æ‰€æœ‰æ–‡ä»¶å¤¹ [é»˜è®¤: %(default)s]",
    )
    parser.add_argument(
        "--location",
        metavar="<folder>",
        type=str,
        default=None,
        required=None,
        help="å­˜å‚¨æ•°æ®åº“æ–‡ä»¶çš„è·¯å¾„ [é»˜è®¤: %(default)s]",
    )
    parser.add_argument(
        "--paths",
        metavar="<file>",
        type=str,
        help="è·¯å¾„çš„ä½å›¾æˆ–åŒ…å«è·¯å¾„çš„æ–‡ä»¶ (å‚è€ƒ paths.example)",
    )

    args = parser.parse_args()
    if args.debug:
        logging.getLogger("emd").setLevel(logging.DEBUG)
    logging.info("*** xiaoya_emd version 1.6.15 ***")
    paths = []
    if args.all:
        paths = s_paths_all
        s_pool.pop(0)
        if args.purge:
            args.db = True
    else:
        if args.paths:
            paths_from_file = []
            is_bitmap = False
            try:
                paths_bitmap = int(args.paths)
                paths_from_file = get_paths_from_bitmap(paths_bitmap, s_paths_all)
                is_bitmap = True
            except ValueError:
                logging.info("è·¯å¾„å‚æ•°ä¸æ˜¯ä½å›¾ï¼Œå°è¯•ä»æ–‡ä»¶åŠ è½½")
            if not is_bitmap:
                paths_from_file = load_paths_from_file(args.paths)
            if not paths_from_file:
                logging.error("è·¯å¾„æ–‡ä»¶ä¸åŒ…å«æœ‰æ•ˆè·¯å¾„æˆ–ä½å›¾å€¼é”™è¯¯: %s", args.paths)
                sys.exit(1)
            for path in paths_from_file:
                if not is_subpath(path, s_paths):
                    s_pool.pop(0)
                    break
            paths.extend(paths_from_file)
        if not paths:
            paths = s_paths

    if args.media:
        if not os.path.exists(os.path.join(args.media, "115")):
            logging.warning("115 æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...æ­¤è§£å†³æ–¹æ³•å°†åœ¨ä¸‹ä¸ªç‰ˆæœ¬ç§»é™¤")
            os.makedirs(os.path.join(args.media, "115"))
        if not test_media_folder(args.media, t_paths):
            logging.error("è·¯å¾„ %s ä¸åŒ…å«æ‰€éœ€æ–‡ä»¶å¤¹ï¼Œè¯·ä¿®æ­£ --media å‚æ•°", args.media)
            sys.exit(1)
        else:
            media = args.media.rstrip("/")
    if not args.url:
        url = pick_a_pool_member(s_pool)
    else:
        url = args.url
    if urlparse(url).path != "/" and (args.purge or args.db):
        logger.warning("--db æˆ– --purge ä»…æ”¯æŒæ ¹è·¯å¾„æ¨¡å¼")
        sys.exit(1)
    if not url:
        logger.info("æ— æ³•è¿æ¥åˆ°ä»»ä½•æœåŠ¡å™¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥...")
        sys.exit(1)
    if urlparse(url).path == "/":
        total_amount = current_amount(url + ".scan.list.gz", media, paths)
        logger.info("%s ä¸­æœ‰ %d ä¸ªæ–‡ä»¶", url, total_amount)
    semaphore = asyncio.Semaphore(args.count)
    db_session = None
    if args.db or args.purge:
        assert sys.version_info >= (3, 12), "æ•°æ®åº“åŠŸèƒ½éœ€è¦ Python 3.12+"
        if args.location:
            if test_db_folder(args.location) is True:
                db_location = args.location.rstrip("/")
            else:
                sys.exit(1)
        else:
            db_location = media
        localdb = os.path.join(db_location, ".localfiles.db")
        tempdb = os.path.join(db_location, ".tempfiles.db")
        if not os.path.exists(localdb):
            await generate_localdb(localdb, media, paths)
        elif args.db:
            os.remove(localdb)
            await generate_localdb(localdb, media, paths)
        else:
            async with aiosqlite.connect(localdb) as local_session:
                local_amount = await get_total_items_count(local_session)
                if (
                    local_amount > 0
                    and total_amount > 0
                    and abs(total_amount - local_amount) > 1000
                ):
                    logger.warning("æœ¬åœ°æ•°æ®åº“ä¸å®Œæ•´ï¼Œæ­£åœ¨é‡æ–°ç”Ÿæˆ...")
                    await local_session.execute("DELETE FROM files")
                    await local_session.commit()
                    await generate_localdb(localdb, media, paths)
        db_session = await aiosqlite.connect(tempdb)
        await create_table(db_session)
    logger.info("å¼€å§‹ç¼“æ…¢çˆ¬å–...")
    async with ClientSession(
        connector=TCPConnector(ssl=False, limit=0, ttl_dns_cache=600),
        timeout=aiohttp.ClientTimeout(total=36000),
    ) as session:
        await bulk_crawl_and_write(
            url=url,
            session=session,
            db_session=db_session,
            semaphore=semaphore,
            media=media,
            nfo=args.nfo,
            paths=paths,
        )
    if db_session:
        await db_session.commit()
        await db_session.close()
    if args.purge:
        await purge_removed_files(localdb, tempdb, media, total_amount)
        remove_empty_folders(paths, media)
        os.remove(localdb)
        if not args.all:
            os.rename(tempdb, localdb)
        else:
            os.remove(tempdb)
    logger.info("å®Œæˆ...")

if __name__ == "__main__":
    assert sys.version_info >= (3, 10), "è„šæœ¬éœ€è¦ Python 3.10+"
    asyncio.run(main())
